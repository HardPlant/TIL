#

## 텍스트 게임 에이전트

* 현재 => 비디오 게임, 로보틱스

* 대화 시스템 .. 학습환경 찾기 쉽지 않음

* TextWorld 환경

### 텍스트월드 게임

CLI 입력, 턴 기반 게임

* 파서 기반

* Inform 등 개발 툴이 있음

### 강화학습에서는..

* 부분 관찰 .. 항상 환경의 부분만 볼 수 있음
MDP 적용 불가

* 큰 상태공간

Action .. 큼

* 크고 희소한 행위 공간
Directed, Guided 

* 탐색

long-term path, sparse한 reward

* 언어 생성

agent .. 
언어 이해 .. 관찰상태 자연어, 게임마다 다른 파서
언어 습득 .. 대상들과 상호작용방법 학습, 상황 추론
언어 생성 .. command

### TextWorld

* 게임 자동 생성, 시뮬레이션 가능

* OpenAI Gym 호환

* 지식기반을 정의하고 방, 퀘스트를 생성해 줌

curriculum learning

LSTM-DQN .. 복잡해지면 DQN으로 커버할 수 없음
LSTM-DRQN
KG-DQN
Adaptive Action Space .. DQN을 벗어나 짖Model b RNN

###

g/DeepReward

## 계층형 강화학습 & 챗봇

계층형/베이지언/멀티Modal

Hierichial RL

긴 과정 .. Subgoal이 필요한 task를 해결

* 4가지..추상화된 간격이 있을 때

* Feudal Learning .. Manager가 모든 DK를 알아야

* Options Frameworkㅗ

MDP .. 
SMDP .. 

Options .. Semi MDP, MDP가 놓치는 세부 정보를 줌

* HAM

* MAXQ

### Subtask 문제

NN 사용

어느 옵션을 사용하고 있는지, 옵션이 끝났는지? (Option Critic Arch)

* Termination CA

종료조건 판단

* 

* Data를 많이 쓰게 되어 있음
경험 데이터 - 결과 데이터에 대한 재라벨링 필요

* H-Imitation / RL

모방학습 (피드백 기반)
행위에 직업 피드백을 주는 대신 상단에 피드백

* input -> Goal-encoder =>  =>policy -> output

###

subgoal이 이상행위에 가까움, k-means로 찾은 뒤
cnn으로 인코딩

###

H/Interpretable Skill Acquisition @ Multitask RL
복잡한 정책 적용

### Subgoal 자동 생성

### Infobot .. Tx/Explo via Info Bottleneck

goal을 풀기 위한 policy에서 default-policy를 찾는 노력

정보에 비대칭이 존재하는가?

### policy distillation

teacher - student 구조

teacher보다 student가 더 

### NL

### Chatbot

End-to-end

* RL .. Action Space

Dialog Action + Slot

req ( )

* 게층형으로 sub-domain 정의하는 게 배꼽이 더 클 수 있음

* ConvLab

### 어려움

유저도 계층형이 됨

H/User Simulator

BERT ..  

## 게임개발과 강화학습

netmarble 마젤란 팀 .. narc

* 불확실성 .. 자기가 

* 밸런싱

* pvp 매칭 문제

=> 잘 하는 에이전트

### 게임 월드

에이전트 만들기
..강화학습, 모방학습, heuristic (룰베이스)

밸런싱 테스트 지표 .. 

룰기반 AI + 강화학습 에이전트 이용

### 프로세스

네트워크 구성 -> -> 셀프 플레이 ->

####

user info => CNN
card info => CNN

### 어떤 카드를 어디에 배치할 것인가?

feature ..
spacial/non-spacial

첫번쨰 액션 output -> 두번째 액션 input

### 플레이 방법, 승리조건 정의

### 시뮬레이터 구축

### feature 정의

action space 정의

* 목표에 랜덤이 붙으면 학습이 힘듬 (주사위)

* 한 턴에 여러 행동 가능

deadline 필요

* 한 액션이 다음 액션에 영향

{CNN, FC} => FC => LSTM => FC

* A2C 알고리즘
모든 Action을 모아 하나의 네트워크 Output으로 구성

### action filter

한번의 step에 여러 action, 원하는 액션의 결과만 취합해 loss 반영 가능

해당 상황에서 할 수 없는 action (사다리타기) ..
디테일해야 함

학습 속도를 빠르게 해야 함

### 

