# Netflix OSS, Spring Cloud 를 활용한 MSA 구축 및 개발

[](https://github.com/Gunju-Ko/spring-cloud-workshop)

* 빌드 배포 시스템

* 프로젝트에 CI/CD 적용

### 예전 배포 (without aws)

scp / tomcat-manager(메모리 문제 있음)
클린 배포(down/deploy/up)
dns 설정..

* 무중단 배포 불가능

##### HA 구성 (High-Availability)

로드밸런서, 여러 대 톰캣

LB out ()-> tomcat down -> deliver -> tomcat up -> LB in

LB - L4 switch, L7 (HAProxy)

##### 동접 1000명 이상

LB
배포 방식 변경 : Jenkins, Ansible, Chef

##### 조직 분리

단일 Repo

* Merge시 Conflict

* QA

* 팀마다 다른 일정, 배포 이슈

push 전담 인력/배포 전담 인력이 생김

* 팀별 도메인 분리, Repo 분리

그래도 공통 부분을 담는 Share.jar 레포지토리가 있어야 함

정기배포시 각 프로젝트에서 compile `'kr.co.12st:share:1.0.0-SNAPSHOT'`

(SNAPSHOT 버전이 붙어 있으면 mvn에서 라이브러리 다시 다운받음)

여전히 정기배포 필요 (share.jar)

* 소스코드가 수백만 줄이 되면 윈도우에서 로드하기 힘들어짐

* 코드가 어디서 쓰이는지 알 수 없음.. copy&paste

usage만으로는 부족함, 리플렉션 호출은 못 잡음

### 콘웨이의 법칙

(광범위)

### 모놀리틱 아키텍쳐

* 개발 단순

* 배포 단순

* Scale-out 단순

.. DB가 1개이므로 한계가 있음
한계가 오면 MSA로 넘어감

##### 단점

* 무거움

* App 시작이 오래 걸림

* 책임 한계, 소유권 불투명해짐

개발자 만족도가 떨어짐

### 회사의 결정

* 빅뱅 / 차세대 프로젝트(은행권)

NetScape가 이러다 망함

* MSA 플랫폼을 구축하며 고사시킴

### MSA

아마존/넷플릭스
제프 베조스가 아버지나 다름 없음 (2002년 메일)
잡스를 뛰어넘는 마이크로매니징의 1인자라고 함

AWS 릴리즈 (2006년), 내부 플랫폼과 똑같음

##### 넷플릭스

2008년에 SPoF 발생
2009년에 AWS 이관함, 확장성/성능/가용성

* On-premise에 비해 18배 비용 감소시킴

##### 공식적인 정의

없음

* 공감대

3~9명 이상의 팀일 때 쪼개기 가능
9명이 12개 MSA를 만들면...
(클라우드 네이티브 없이) -> 문제 파악이 안됨, 장애 증가

아직은 아이디어 수준임

##### Mono / MSA 비교

share.jar 제거 (MSA는 코드 공유를 하지 않음)
DB도 공유하지 않음 (팀별 독립 db)

서비스에서 rest 호출

DB는 가장 분리하기 힘든 부분임
큰 데이터베이스를 MSA 데이터베이스로 어떻게 분리할 것인가?
(MSA의 꽃임)

DB 분리는 분산 트랜잭션, 이벤트주도도 가야함

* 쿠팡 케이스 ... 아마존과 비슷한 방법론

다른 팀의 DB에 접근하면 안됨
replica로 DB를 생성하든지

* 이벤트 드리븐(linkedin 방식)
captureDirChange / DB에 저장된 트랜잭션 기록을 모두 읽어 Kafka에 쏨
받은 이벤트를 하나하나 분리함

* MSA는 코드 중복을 허용함, API 스펙이 견고하게 됨

매주 API에 대한 변경 여부 등 커뮤니티를 만들어야 함

### 실습

* Display(전시 영역, 8081) -> 상품 호출(8082)

```
request : /displays/{diplayId}
response : "[product id = " + productId + " at " + System.currentTimeMillis() + "]";
```

### Cloud Native

위치는 중요하지 않음
애플리케이션은 어떻게 만들고 배포하는가?

### DevOps

* 전통적 모델

개발/운영 분리
일을 던지고 잊어버림 (UDP)

* DevOps

"You run it, you build it" 만들면 운영까지 해야 함

팀이 프로젝트 그룹 아닌 제품 그룹에 소속

##### Twelve Factors

Heroku 클라우드 플랫폼 창시자들이 만듬
지키면 MSA하기 편해짐
프로젝트 진행 시 프로젝트 구조, 배포 구조를 잡을 때 참고하면 편함

* 핵심 사상

선언적 - 무엇을 해야 한다 (명령적, 어떻게 해야 하는지가 아님)
이식성 - OS에 구애받지 않도록
클라우드 플랫폼 기반 개발
지속적 배포
수직적 확장

* 

* 

* 

* 

* 백엔드(지원) 서비스

backing 서비스는 필요에 따라 추가되는 자원
환경마다 바꿀 수 있도록

* 빌드, 릴리즈, 실행

빌드/실행을 분리해야 함
빌드 -> Jenkins, 하나의 패키지를 만듬
릴리즈 -> 빌드에 환경설정 정보 **조합**, 젠킨스 아티팩트, 시맨틱 버저닝(Netflix nebular)등 식별자
실행 -> 릴리즈 버전 중 하나를 실행

* 포트 바인딩

포트에 연결해 외부에 공개
11st에서도 임베디드 톰캣을 쓰고 있음
실행 환경에 웹 서버를 따로 추가해줄 필요 없음

* 동시성
(Histrix)

App이 필요할 때마다 Process/Thread 만들어 병렬적으로

HTTP 요청 - 서블릿 쓰레드가 처리
시간이 오래 걸리는 작업은 워커 쓰레드가 처리

(톰캣 기본설정 : 서블릿 쓰레드 - 200개)

* 차분성

graceful하게 shutdown되야 함

사용하는 리소스가 정리되는가

* dev/prod 일치

최대한 비슷하게
3가지 차이
* 시간
* 개인
* 도구 

L4 : 보통 상용에서만 씀, 테스트가 될 수 없음
개발 L4를 붙여도 상용과 동일하게 하기 쉽지 않음

* 

* 로그

애플리케이션은 로그 파일 저장에 관여하지 않아야 함
로그를 중앙에 저장

* Admin 프로세스

admin 코드도 app 코드와 함께 git에 있어야 함

* **제약사항임**

하나하나 지키면 좋음

### REST API

행위에 집중

* Monolitic Lifecycle을 보았음

MSA는 성능으로 설득되지는 않음
캐시/DB 레플리카로 가능한가?
코드 Onwership, 책임소재 불분명 등

### Spring Cloud

* Hystrix - Circuit breaker

* Eureka - Service Discovery

* Zuul - API Gateway

Netflix OSS(RxJava도 여기서 만듬) / Spring Cloud의 교집합

가운데에 있는 것을 쓸 것임

### Hystrix

##### 모놀리틱 의존성 호출

```
digraph {
    Controller -> Service
    Service -> Repository

}
```

100% 신뢰됨

##### Failure as a First Class Citizen

일급객체, 가장 먼저 생각해야 함

클라우드에서는 실패가 일급시민임 (모놀리틱에는 없었음)

가동률이 99.99%
이상적인 uptime에서도..

10억 요청 중 0.3% 실패 .. 300만 요청
매달마다 2시간 이상 downtime

##### Hystrix

분산시스템의 지얀/실패 용인성 제공

* Tomcat 기분 설정 .. 200 threads, no timeout

의존성 맨 끝에 있는 tomcat에서 오류가 발생하면 무한정 timeout하면서 줄줄히 대기하는 상황 나옴

결국 사용자는 빈 화면 보게 됨
(11번가 아닌 어떤 서비스는 ...)

##### Hystrix 적용

`@HystrixCommand` 어노테이션

* 호출을 인터럽트해서 대신 실행

* 성공/실패 여부(예외), 통계를 냄

* 통계에 따라 Circuit Open 여부 판단, 조치를 취함

###### Circuit Open 시 모든 요청에 즉시 에러 (지연이 있으므로)

특정 메서드 지연이 시스템 전체 리소스(Thread, Memory...)를 모두 소모할 수 있음

장애 유발 시스템에 대한 연동을 조기 차단 (Fail fast)

백프리셔 .. 

###### 기본 설정

10초동안 20개 이상 호출 시 검사, 50% 이상 실패 시 Circuit Open

###### 조치 취하기

Fallback .. Circuit Open 시 / 기타 실패 시 대신 호출됨
Exception 대신 응답할 기본 구현 넣기

###### TimeOut

오랫동안 응답이 없는 메서드 .. 별도 쓰레드로 격리
Fallback으로 가게 함

* 기본 설정 .. 1초

* 설정 시간동안 메서드가 끝나지 않으면 실제 실행 인터럽트, 이후 타임아웃 예외

* Open 후 5초 뒤 호출 한번 해서 정상적이면 (Half Open) 정상화 (Close), 

### Hystrix 실습

* gradle에 hystrix 의존성 추가

* Application에 `@EnableCircuitBreaker` 어노테이션 추가

* 서비스 구현체의 rest 호출 메서드에 `@HystrixCommand` 추가

* Fallback 메서드 추가 (원래 메서드와 같은 반환값을 갖도록)

* `@HystrixCommand` 어노테이션의 fallbackMethod에 지정

아직 circuit open 상태는 아님

##### 내부 동작 방식

* HystrixCommand에 의해 실행됨

##### Throwable 추가

Fallback 메서드 시그니처에 `Throwable` 추가

### 실습 - Timeout 정의

2초로 정의하기

* config 기반임

```yaml
hystrix:
  command:
    default:
      execution:
        isolation:
          thread:
            timeoutInMilliseconds: 3000
```

command 하위에서 default 이외 값을 추가 가능

### 실습 - Hystrix Circuit Open 테스트

* 기본 설정 (테스트 힘듬)

20회 요청, 10회 실패 시 서킷 Open

* 변경설정

```yaml
hystrix:
  command:
    productInfo:
      execution:
        isolation:
          thread:
            timeoutInMilliseconds: 3000
      circuitBreaker:
        requestVolumeThresold: 1 # default 50
        errorThresholdPercentage: 50 # default 50
```

자비가 없도록

워커 쓰레드는 따로 관리하도록 하는 것을 `@HystrixCommand`로 처리
서블릿스레드의 빠른 리턴 후 워커 쓰레드 작업

실패 시 Open 상태 연장..

```yaml
        sleepWindowInMilliseconds : 5000
```

##### 실습 - commandKey의 단위

command 하위 -> 어노테이션에 매핑

##### Hystrix

ThreadPool 사용이 가능함

##### 동시 실행 Command 수 제한 - ThreadPool 설정

`@HystrixCommand`로 선언된 메서드는 별도 ThreadPool에서 대신 실행됨
ThreadPool 설정을 하지 않으면 클래스 이름을 기본으로 함
`commandGroupKey` 옵션으로 수정 가능

##### 튜닝

`@HystrixProperty` 어노테이션 사용

coreSize는 기본 10임, 시간이 오래 걸리는 서비스라면 늘리는 걸 추천

### Eureka

##### Server Side LoadBalancer

##### Client Side LB

API Caller가 서버 목록에 대해 LB 수행

* Ribbon

S/W 로드밸런싱
서버 목록 동적 변경
LB Schema 마음대로 구성 가능

### 실습 - RestTemplate에 Ribbon 적용

### 실습 - Ribbon Retry

* 리트라이가 적용되냐, 안 되냐?

디버그

### Ribbon의 서버 목록을 yml에 넣는 것보다 자동화하는 방법

서버 시작/종료 시 서비스 디스커버리

### Dynamic Service Discovery

* Service Registry

서비스 탐색, 등록
(침투적 방식 코드)

추상화(인터페이스로 뺐음)

* DiscoveryClient

#### Eureka 적용

* Spring Cloud에서

서버 시작 시 자신의 상태 등록

주기적 HeartBeat로 Server에 살아있음을 알림

서버 종료 시 Eureka Server에 상태 변경/삭제

실제 등록명..

* 디폴트로 실제 서비스에서 굴러가고 있음

* Eureka 서버 명시

```yaml
eureka:
  instance:
    prefer-ip-address: true
  client:
    service-url:
      defaultZone: http://127.0.0.1/eureka
```

이중화 필수임 (MSA에서 중요)

30초마다 서버 목록을 로컬과 동기화함
Ribbon이 Discovery 클라이언트를 통해 url 목록 변경함

### 정리

* 코드에서 product로 접근
* 

### Feign

Ribbon/RestTemplate는 기술적으로 좋지만 프로그래밍적으로는 좋지 않음 (명령형)
스펙, 사용방법을 알고 있어야 함

RestTemplate는 테스트하기 어려움

* 관심사 분리

리소스, 서비스 호출, ...

##### 선언적 Http Client

인터페이스 선언으로 Client 구현물을 만들어 줌

### 실습 - Feign 클라이언트 사용

### Feign + Hystrix, Ribbon, Eureka

내부적으로 통합되어 있음

`@FeignClient`에 URL 명시 시.. 순수 Feign Client

명시하지 않으면 Feign + Ribbon + Eureka 모드로 동작

* Fallback - 

* Hystrix 설정 - 

### 기본 Fallback은 에러 원인 알 수 없음

Factory로 만들어 Throwable 캐치

### Zuul - API Gateway

선언적으로 가능함
외부에서는 URL 하나는 알아야 함

API Gateway front door

* 횡단 관심사

보안, 인증, 인가

일정량 이상의 요청 제한

계측(metering)

이런 것들을 해결해줌

### Netflix Zuul

마이크로 프록시

* 멀티리전

리전 장애 -> 다른 리전으로 이동

* 무조건 이중화, 앞에 ELB/L4

모든 API 요청을 HystrixCommand로 구성

API 경로별 Circuit Breaker 생성

* 내부적으로 Ribbon을 통해 로드밸런싱

* 서버 리스트

* 16개 정도면 11st 서비스를 굴릴 수 있음


### 실습

* Zuul - Eureka를 통해 관리

* round robin 사용 호출

### Spring Cloud Zuul

Netflix Zuul과는 많이 다름

기본 Isolation이 세마포어임 (Netflix는 ThreadPool)

세마포어는 network timeout을 격리시켜주지 못함

실제 세마포어가 5면 톰캣 쓰레드풀 상관없이 5임

`zuul.ribbon-isolation-strategy: thread`

모든 리본커맨드가 하나 쓰레드풀에 들어감
패치됨

```yaml
zuul.thread-pool.useSeparateThreadPools: true
zuul.thread-pool.threadPoolKeyPrefix: zuul-
```

옵션을 통해 쓰레드풀로 실행

##### 타임아웃 지정

* Hystrix와 별개

* REST 타임아웃 지정하듯 Ribbon 타임아웃

16대로 15000TPS 받아내기

##### 교차관심사

로그 (에러율 분석, 에러 필터 등)


### Spring Cloud Config

* 중앙화 된 설정 서버

refresh 날렸을때 config 빈이 재생성됨

재배포 없이 리스타트 시 

refresh scope이 상용 서비스에서 적절한가?

* 설정이 git 기반

pull request로 리뷰 가능

### Spring Boot Admin, Turbine

대시보드
엑티브 스레드, 

### Zipkin / Spring Cloud Sleuth

서비스 많고, 인스턴스는 곳곳에 분포됨
rc -> 다른 서비스를 얼마나 호출했나 추적함

### 다루지 못한 이야기...

분산 트랜잭션
보상 트랜잭션
이벤트 드리븐 아키텍쳐

* 코드 리뷰

100% 온라인도 가능함
Pull Request를 통해, 코드 검사 (Sonarqube), 디폴트 리뷰어 정책
